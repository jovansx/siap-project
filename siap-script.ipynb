{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovansx/siap-project/blob/develop/siap-script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "C4LuZ1dDtQN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "GZQ4BT2DtkkL",
        "outputId": "1b82b39d-7590-48c3-bfd4-dd701c524fc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "knUWnFxN71cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_champion_json_to_map():\n",
        "  df_champs = pd.read_json('/content/drive/MyDrive/SIAP Data/champion_info.json')\n",
        "  champ_map = {}\n",
        "\n",
        "  for ch in df_champs.data:\n",
        "    champ_map[ch[\"id\"]] = ch[\"name\"]\n",
        "  return champ_map\n",
        "\n",
        "def generate_df2(champ_map, champ_id, champ_name, df1):\n",
        "  df2 = pd.DataFrame(list(champ_map.items()), columns = [champ_id,champ_name])\n",
        "  merged = pd.merge(df1, df2, on=champ_id)\n",
        "  dropped = merged.drop(columns=[champ_id], axis=1)\n",
        "  return dropped\n",
        "\n",
        "def merge(champ_map, df1):\n",
        "  df1 = generate_df2(champ_map, 't1_champ1id', 't1_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ2id', 't1_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ3id', 't1_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ4id', 't1_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ5id', 't1_champ5name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ1id', 't2_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ2id', 't2_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ3id', 't2_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ4id', 't2_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ5id', 't2_champ5name', df1)\n",
        "  return df1\n",
        "\n",
        "def delete_columns(df, columns_deletion):\n",
        "  return df.drop(columns=columns_deletion, axis=1)\n",
        "\n",
        "def preprocess_dataframe(df_stats):\n",
        "  df_stats = delete_columns(df_stats, [\"Class\", \"Role\", \"Tier\", \"Trend\", \"Role %\", \"Pick %\", \"Ban %\", \"KDA\"])   # Remove columns\n",
        "  df_stats['Win %'] = df_stats['Win %'].str[:5].astype(float)                                                   # Remove % and convert to float\n",
        "  distinct_names = set(df_stats['Name'])                                                                        # Distinct names\n",
        "  new_df_stats = pd.DataFrame({'Name': [], 'Score': [], 'Win': []})                                             # New empty dataframe of stats\n",
        "\n",
        "  for name in distinct_names:\n",
        "    sub_df_stats = df_stats.loc[df_stats['Name'] == name]\n",
        "    score = sub_df_stats[\"Score\"].mean(axis=0)\n",
        "    win_rate = sub_df_stats[\"Win %\"].mean(axis=0)\n",
        "    one_row_df = pd.DataFrame({\"Name\": [name], \"Score\": [score], \"Win\": [win_rate]})\n",
        "    new_df_stats = new_df_stats.append(one_row_df)                                                              # Append average values for every champion\n",
        "  new_df_stats.index = range(1,len(new_df_stats)+1)                                                             # Set incremental indexes\n",
        "  return new_df_stats\n",
        "\n",
        "def merge_games_with_stats_helper(df_games, df_stats, old_score, new_score, old_win, new_win, name):\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_score, new_score)\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_win, new_win)\n",
        "  df_games.columns = df_games.columns.str.replace(name, \"Name\")\n",
        "  df_games = pd.merge(df_games, df_stats, on=\"Name\")\n",
        "  df_games = df_games.drop(columns=[\"Name\"], axis=1)\n",
        "  return df_games, df_stats\n",
        "\n",
        "def merge_games_with_stats(df_games, df_stats):\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"Score\", \"t1_champ1score\", \"Win\", \"t1_champ1win\", \"t1_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ1score\", \"t1_champ2score\", \"t1_champ1win\", \"t1_champ2win\", \"t1_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ2score\", \"t1_champ3score\", \"t1_champ2win\", \"t1_champ3win\", \"t1_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ3score\", \"t1_champ4score\", \"t1_champ3win\", \"t1_champ4win\", \"t1_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ4score\", \"t1_champ5score\", \"t1_champ4win\", \"t1_champ5win\", \"t1_champ5name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ5score\", \"t2_champ1score\", \"t1_champ5win\", \"t2_champ1win\", \"t2_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ1score\", \"t2_champ2score\", \"t2_champ1win\", \"t2_champ2win\", \"t2_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ2score\", \"t2_champ3score\", \"t2_champ2win\", \"t2_champ3win\", \"t2_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ3score\", \"t2_champ4score\", \"t2_champ3win\", \"t2_champ4win\", \"t2_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ4score\", \"t2_champ5score\", \"t2_champ4win\", \"t2_champ5win\", \"t2_champ5name\")\n",
        "  return df_games\n",
        "\n",
        "def create_scaler(df_games):\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit(df_games)\n",
        "  # return (df_games-df_games.mean())/df_games.std()\n",
        "\n",
        "def extract_y_from_dataframe(df_games):\n",
        "  df_games_y = df_games[\"winner\"]\n",
        "  df_games = df_games.drop(columns=[\"winner\"], axis=1)\n",
        "  return df_games, df_games_y\n"
      ],
      "metadata": {
        "id": "_f9eTm-Z74EE"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main "
      ],
      "metadata": {
        "id": "7qjAM_yx8cRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "df_games = pd.read_csv('/content/drive/MyDrive/SIAP Data/games.csv')\n",
        "df_stats = pd.read_csv('/content/drive/MyDrive/SIAP Data/League of Legends Champion Stats 12.1.csv', delimiter=';')\n",
        "\n",
        "# Replace champion ids with names\n",
        "champ_map = convert_champion_json_to_map()\n",
        "df_games = merge(champ_map, df_games)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_games = delete_columns(df_games, [\"gameId\", \"creationTime\", \"seasonId\", \"firstBlood\", \"firstTower\", \"firstInhibitor\", \"firstBaron\",\n",
        "                                     \"firstDragon\", \"firstRiftHerald\", \"t1_towerKills\", \"t1_inhibitorKills\", \"t1_baronKills\", \"t1_dragonKills\",\n",
        "                                     \"t1_riftHeraldKills\", \"t2_towerKills\", \"t2_inhibitorKills\", \"t2_baronKills\", \"t2_dragonKills\", \"t2_riftHeraldKills\"])\n",
        "df_games = delete_columns(df_games, [\"t1_champ1_sum1\", \"t1_champ1_sum2\", \"t1_champ2_sum1\", \"t1_champ2_sum2\", \"t1_champ3_sum1\", \"t1_champ3_sum2\",\n",
        "                                     \"t1_champ4_sum1\", \"t1_champ4_sum2\", \"t1_champ5_sum1\", \"t1_champ5_sum2\", \"t2_champ1_sum1\", \"t2_champ1_sum2\",\n",
        "                                     \"t2_champ2_sum1\", \"t2_champ2_sum2\", \"t2_champ3_sum1\", \"t2_champ3_sum2\", \"t2_champ4_sum1\", \"t2_champ4_sum2\",\n",
        "                                     \"t2_champ5_sum1\", \"t2_champ5_sum2\"])\n",
        "\n",
        "# Preprocess data of df_stats\n",
        "df_stats = preprocess_dataframe(df_stats)\n",
        "\n",
        "# Merge read data into single dataframe\n",
        "df_games = merge_games_with_stats(df_games, df_stats)\n",
        "\n",
        "# Shuffle data in dataframe\n",
        "df_games = df_games.sample(frac = 1)\n",
        "\n",
        "# Extract y values into separate dataframe, and drop it from existing one\n",
        "df_games_x, df_games_y = extract_y_from_dataframe(df_games)\n",
        "\n",
        "# Split data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_games, df_games_y, test_size = 0.30)\n",
        "\n",
        "# Create Standard scaler based on X_train -> transform X_train and X_test using that scaler\n",
        "scaler = create_scaler(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create model\n",
        "clf = RandomForestClassifier(n_estimators = 100) \n",
        "# clf = LinearSVC(random_state=0, tol=1e-5)\n",
        "# clf = LogisticRegression(max_iter=10000)\n",
        "# clf = tree.DecisionTreeClassifier()\n",
        "\n",
        "# Train model using X_train and y_train\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8kHhsLjE8e_i",
        "outputId": "ed24c5d1-9a83-4903-cffa-c513f23bc0fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY OF THE MODEL:  1.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}