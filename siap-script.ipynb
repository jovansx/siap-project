{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovansx/siap-project/blob/develop/siap-script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
        "from ydata_profiling import ProfileReport"
      ],
      "metadata": {
        "id": "GZQ4BT2DtkkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "knUWnFxN71cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MinMaxScaler,\n",
        "def convert_champion_json_to_map():\n",
        "  df_champs = pd.read_json('/content/drive/MyDrive/SIAP Data/champion_info.json')\n",
        "  champ_map = {}\n",
        "\n",
        "  for ch in df_champs.data:\n",
        "    champ_map[ch[\"id\"]] = ch[\"name\"]\n",
        "  return champ_map\n",
        "\n",
        "def generate_df2(champ_map, champ_id, champ_name, df1):\n",
        "  df2 = pd.DataFrame(list(champ_map.items()), columns = [champ_id,champ_name])\n",
        "  merged = pd.merge(df1, df2, on=champ_id)\n",
        "  dropped = merged.drop(columns=[champ_id], axis=1)\n",
        "  return dropped\n",
        "\n",
        "def merge(champ_map, df1):\n",
        "  df1 = generate_df2(champ_map, 't1_champ1id', 't1_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ2id', 't1_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ3id', 't1_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ4id', 't1_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ5id', 't1_champ5name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ1id', 't2_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ2id', 't2_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ3id', 't2_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ4id', 't2_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ5id', 't2_champ5name', df1)\n",
        "  return df1\n",
        "\n",
        "def delete_columns(df, columns_deletion):\n",
        "  return df.drop(columns=columns_deletion, axis=1)\n",
        "\n",
        "def preprocess_dataframe(df_stats):\n",
        "  df_stats = delete_columns(df_stats, [\"Class\", \"Role\", \"Tier\", \"Trend\", \"Role %\", \"Pick %\", \"Ban %\", \"KDA\"])   # Remove columns\n",
        "  df_stats['Win %'] = df_stats['Win %'].str[:5].astype(float)                                                   # Remove % and convert to float\n",
        "  distinct_names = set(df_stats['Name'])                                                                        # Distinct names\n",
        "  new_df_stats = pd.DataFrame({'Name': [], 'Score': [], 'Win': []})                                             # New empty dataframe of stats\n",
        "\n",
        "  for name in distinct_names:\n",
        "    sub_df_stats = df_stats.loc[df_stats['Name'] == name]\n",
        "    score = sub_df_stats[\"Score\"].mean(axis=0)\n",
        "    win_rate = sub_df_stats[\"Win %\"].mean(axis=0)\n",
        "    one_row_df = pd.DataFrame({\"Name\": [name], \"Score\": [score], \"Win\": [win_rate]})\n",
        "    new_df_stats = new_df_stats.append(one_row_df)                                                              # Append average values for every champion\n",
        "  new_df_stats.index = range(1,len(new_df_stats)+1)                                                             # Set incremental indexes\n",
        "  return new_df_stats\n",
        "\n",
        "def merge_games_with_stats_helper(df_games, df_stats, old_score, new_score, old_win, new_win, name):\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_score, new_score)\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_win, new_win)\n",
        "  df_games.columns = df_games.columns.str.replace(name, \"Name\")\n",
        "  df_games = pd.merge(df_games, df_stats, on=\"Name\")\n",
        "  df_games = df_games.drop(columns=[\"Name\"], axis=1)\n",
        "  return df_games, df_stats\n",
        "\n",
        "def merge_games_with_stats(df_games, df_stats):\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"Score\", \"t1_champ1score\", \"Win\", \"t1_champ1win\", \"t1_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ1score\", \"t1_champ2score\", \"t1_champ1win\", \"t1_champ2win\", \"t1_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ2score\", \"t1_champ3score\", \"t1_champ2win\", \"t1_champ3win\", \"t1_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ3score\", \"t1_champ4score\", \"t1_champ3win\", \"t1_champ4win\", \"t1_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ4score\", \"t1_champ5score\", \"t1_champ4win\", \"t1_champ5win\", \"t1_champ5name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ5score\", \"t2_champ1score\", \"t1_champ5win\", \"t2_champ1win\", \"t2_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ1score\", \"t2_champ2score\", \"t2_champ1win\", \"t2_champ2win\", \"t2_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ2score\", \"t2_champ3score\", \"t2_champ2win\", \"t2_champ3win\", \"t2_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ3score\", \"t2_champ4score\", \"t2_champ3win\", \"t2_champ4win\", \"t2_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ4score\", \"t2_champ5score\", \"t2_champ4win\", \"t2_champ5win\", \"t2_champ5name\")\n",
        "  return df_games\n",
        "\n",
        "def create_scaler(df_games):\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit(df_games)\n",
        "  # return (df_games-df_games.mean())/df_games.std()\n",
        "\n",
        "def extract_y_from_dataframe(df_games):\n",
        "  df_games_y = df_games[\"winner\"]\n",
        "  df_games = df_games.drop(columns=[\"winner\"], axis=1)\n",
        "  return df_games, df_games_y\n"
      ],
      "metadata": {
        "id": "_f9eTm-Z74EE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main "
      ],
      "metadata": {
        "id": "7qjAM_yx8cRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "df_games = pd.read_csv('/content/drive/MyDrive/SIAP Data/games.csv')\n",
        "df_stats = pd.read_csv('/content/drive/MyDrive/SIAP Data/League of Legends Champion Stats 12.1.csv', delimiter=';')\n",
        "\n",
        "# Replace champion ids with names\n",
        "champ_map = convert_champion_json_to_map()\n",
        "df_games = merge(champ_map, df_games)\n",
        "\n",
        "# Preprocess data of df_stats\n",
        "df_stats = preprocess_dataframe(df_stats)\n",
        "\n",
        "# Merge read data into single dataframe\n",
        "df_games = merge_games_with_stats(df_games, df_stats)\n",
        "\n",
        "# Leave chosen columns\n",
        "df_games = df_games[[\"winner\", \"t1_champ1win\", \"t2_champ1win\", \"t1_champ2win\", \"t2_champ2win\", \"t1_champ3win\", \"t2_champ3win\", \"t1_champ4win\", \"t2_champ4win\", \"t1_champ5win\", \"t2_champ5win\",\n",
        "                     \"firstBlood\", \"firstTower\"]]\n",
        "\n",
        "# Shuffle data in dataframe\n",
        "df_games = df_games.sample(frac = 1)\n",
        "\n",
        "# Extract y values into separate dataframe, and drop it from existing one\n",
        "df_games = df_games.drop_duplicates()\n",
        "df_games_x, df_games_y = extract_y_from_dataframe(df_games)\n",
        "\n",
        "# Analyzing data\n",
        "# prof = ProfileReport(df_games_x)\n",
        "# prof.to_file(output_file='output_after_dropping_dragon.html')\n",
        "\n",
        "# Split data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_games_x, df_games_y, test_size = 0.30)\n",
        "\n",
        "# Create Standard scaler based on X_train -> transform X_train and X_test using that scaler\n",
        "scaler = create_scaler(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create models\n",
        "random_forest_classifier = RandomForestClassifier() \n",
        "linear_svc = LinearSVC()\n",
        "logistic_regression = LogisticRegression()\n",
        "decision_tree_classifier = tree.DecisionTreeClassifier()\n",
        "\n",
        "# Train models using X_train and y_train\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "linear_svc.fit(X_train, y_train)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "decision_tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred_1 = random_forest_classifier.predict(X_test)\n",
        "y_pred_2 = linear_svc.predict(X_test)\n",
        "y_pred_3 = logistic_regression.predict(X_test)\n",
        "y_pred_4 = decision_tree_classifier.predict(X_test)\n",
        "\n",
        "print(\"F1 SCORE OF THE RandomForestClassifier: \", metrics.f1_score(y_test, y_pred_1))\n",
        "print(\"F1 SCORE OF THE LinearSVC: \", metrics.f1_score(y_test, y_pred_2))\n",
        "print(\"F1 SCORE OF THE LogisticRegression: \", metrics.f1_score(y_test, y_pred_3))\n",
        "print(\"F1 SCORE OF THE DecisionTreeClassifier: \", metrics.f1_score(y_test, y_pred_4))\n"
      ],
      "metadata": {
        "id": "8kHhsLjE8e_i",
        "outputId": "0235c424-dc62-4c0a-8670-f3bf668f2c29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 SCORE OF THE RandomForestClassifier:  0.7071473750790639\n",
            "F1 SCORE OF THE LinearSVC:  0.7128376251337909\n",
            "F1 SCORE OF THE LogisticRegression:  0.7128912400025191\n",
            "F1 SCORE OF THE DecisionTreeClassifier:  0.5978448001020212\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}