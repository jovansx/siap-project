{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovansx/siap-project/blob/develop/siap-script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
        "from ydata_profiling import ProfileReport\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "GZQ4BT2DtkkL",
        "outputId": "9229eb62-4f61-42c6-d0ab-488585fbfd8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ydata-profiling in /usr/local/lib/python3.9/dist-packages (4.1.2)\n",
            "Requirement already satisfied: statsmodels<0.14,>=0.13.2 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (0.13.5)\n",
            "Requirement already satisfied: tqdm<4.65,>=4.48.2 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (4.64.1)\n",
            "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (6.0)\n",
            "Requirement already satisfied: pydantic<1.11,>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (1.10.7)\n",
            "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (3.1.2)\n",
            "Requirement already satisfied: typeguard<2.14,>=2.13.2 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (2.13.3)\n",
            "Requirement already satisfied: numpy<1.24,>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (1.22.4)\n",
            "Requirement already satisfied: phik<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (0.12.3)\n",
            "Requirement already satisfied: htmlmin==0.1.12 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (0.1.12)\n",
            "Requirement already satisfied: matplotlib<3.7,>=3.2 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (3.6.3)\n",
            "Requirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (1.4.4)\n",
            "Requirement already satisfied: imagehash==4.3.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (4.3.1)\n",
            "Requirement already satisfied: requests<2.29,>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (2.27.1)\n",
            "Requirement already satisfied: seaborn<0.13,>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (0.12.2)\n",
            "Requirement already satisfied: scipy<1.10,>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (1.9.3)\n",
            "Requirement already satisfied: multimethod<1.10,>=1.4 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (1.9.1)\n",
            "Requirement already satisfied: visions[type_image_path]==0.7.5 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling) (0.7.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from imagehash==4.3.1->ydata-profiling) (8.4.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.9/dist-packages (from imagehash==4.3.1->ydata-profiling) (1.4.1)\n",
            "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (0.2.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (3.0)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (22.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas!=1.4.0,<1.6,>1.1->ydata-profiling) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.9/dist-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<1.11,>=1.8.1->ydata-profiling) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling) (2.0.12)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.2->statsmodels<0.14,>=0.13.2->ydata-profiling) (1.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "knUWnFxN71cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MinMaxScaler,\n",
        "def convert_champion_json_to_map():\n",
        "  df_champs = pd.read_json('/content/drive/MyDrive/SIAP Data/champion_info.json')\n",
        "  champ_map = {}\n",
        "\n",
        "  for ch in df_champs.data:\n",
        "    champ_map[ch[\"id\"]] = ch[\"name\"]\n",
        "  return champ_map\n",
        "\n",
        "def generate_df2(champ_map, champ_id, champ_name, df1):\n",
        "  df2 = pd.DataFrame(list(champ_map.items()), columns = [champ_id,champ_name])\n",
        "  merged = pd.merge(df1, df2, on=champ_id)\n",
        "  dropped = merged.drop(columns=[champ_id], axis=1)\n",
        "  return dropped\n",
        "\n",
        "def merge(champ_map, df1):\n",
        "  df1 = generate_df2(champ_map, 't1_champ1id', 't1_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ2id', 't1_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ3id', 't1_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ4id', 't1_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ5id', 't1_champ5name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ1id', 't2_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ2id', 't2_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ3id', 't2_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ4id', 't2_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ5id', 't2_champ5name', df1)\n",
        "  return df1\n",
        "\n",
        "def delete_columns(df, columns_deletion):\n",
        "  return df.drop(columns=columns_deletion, axis=1)\n",
        "\n",
        "def preprocess_dataframe(df_stats):\n",
        "  df_stats = delete_columns(df_stats, [\"Class\", \"Role\", \"Tier\", \"Trend\", \"Role %\", \"Pick %\", \"Ban %\", \"KDA\"])   # Remove columns\n",
        "  df_stats['Win %'] = df_stats['Win %'].str[:5].astype(float)                                                   # Remove % and convert to float\n",
        "  distinct_names = set(df_stats['Name'])                                                                        # Distinct names\n",
        "  new_df_stats = pd.DataFrame({'Name': [], 'Score': [], 'Win': []})                                             # New empty dataframe of stats\n",
        "\n",
        "  for name in distinct_names:\n",
        "    sub_df_stats = df_stats.loc[df_stats['Name'] == name]\n",
        "    score = sub_df_stats[\"Score\"].mean(axis=0)\n",
        "    win_rate = sub_df_stats[\"Win %\"].mean(axis=0)\n",
        "    one_row_df = pd.DataFrame({\"Name\": [name], \"Score\": [score], \"Win\": [win_rate]})\n",
        "    new_df_stats = pd.concat([new_df_stats, one_row_df])                                                        # Append average values for every champion\n",
        "  new_df_stats.index = range(1,len(new_df_stats)+1)                                                             # Set incremental indexes\n",
        "  return new_df_stats\n",
        "\n",
        "def merge_games_with_stats_helper(df_games, df_stats, old_score, new_score, old_win, new_win, name):\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_score, new_score)\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_win, new_win)\n",
        "  df_games.columns = df_games.columns.str.replace(name, \"Name\")\n",
        "  df_games = pd.merge(df_games, df_stats, on=\"Name\")\n",
        "  df_games = df_games.drop(columns=[\"Name\"], axis=1)\n",
        "  return df_games, df_stats\n",
        "\n",
        "def merge_games_with_stats(df_games, df_stats):\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"Score\", \"t1_champ1score\", \"Win\", \"t1_champ1win\", \"t1_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ1score\", \"t1_champ2score\", \"t1_champ1win\", \"t1_champ2win\", \"t1_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ2score\", \"t1_champ3score\", \"t1_champ2win\", \"t1_champ3win\", \"t1_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ3score\", \"t1_champ4score\", \"t1_champ3win\", \"t1_champ4win\", \"t1_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ4score\", \"t1_champ5score\", \"t1_champ4win\", \"t1_champ5win\", \"t1_champ5name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ5score\", \"t2_champ1score\", \"t1_champ5win\", \"t2_champ1win\", \"t2_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ1score\", \"t2_champ2score\", \"t2_champ1win\", \"t2_champ2win\", \"t2_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ2score\", \"t2_champ3score\", \"t2_champ2win\", \"t2_champ3win\", \"t2_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ3score\", \"t2_champ4score\", \"t2_champ3win\", \"t2_champ4win\", \"t2_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ4score\", \"t2_champ5score\", \"t2_champ4win\", \"t2_champ5win\", \"t2_champ5name\")\n",
        "  return df_games\n",
        "\n",
        "def create_scaler(df_games):\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit(df_games)\n",
        "  # return (df_games-df_games.mean())/df_games.std()\n",
        "\n",
        "def extract_y_from_dataframe(df_games):\n",
        "  df_games_y = df_games[\"winner\"]\n",
        "  df_games = df_games.drop(columns=[\"winner\"], axis=1)\n",
        "  return df_games, df_games_y\n",
        "\n",
        "def gird_param_tunining_rfc(X_train, y_train):\n",
        "  param_grid = {\n",
        "      'n_estimators': [100, 200, 300],\n",
        "      'max_depth': [None, 5, 10],\n",
        "      'min_samples_split': [2, 4, 6],\n",
        "      'min_samples_leaf': [1, 2, 4]\n",
        "  }\n",
        "\n",
        "  rfc_grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "  rfc_grid.fit(X_train, y_train)\n",
        "  print(rfc_grid.best_params_)\n",
        "\n",
        "def gird_param_tunining_svc(X_train, y_train):\n",
        "  param_grid = {\n",
        "      'C': [0.1, 1, 10, 100],\n",
        "      'max_iter': [100, 500, 1000, 5000],\n",
        "      'dual': [True, False]\n",
        "  }\n",
        "\n",
        "  svc_grid = GridSearchCV(estimator=LinearSVC(), param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "  svc_grid.fit(X_train, y_train)\n",
        "  print(svc_grid.best_params_)\n",
        "\n",
        "def gird_param_tunining_lr(X_train, y_train):\n",
        "  param_grid = {\n",
        "      'C': [0.1, 1, 10, 100],\n",
        "      'max_iter': [100, 500, 1000, 5000],\n",
        "      'penalty': ['l1', 'l2'],\n",
        "      'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "  }\n",
        "\n",
        "  lr_grid = GridSearchCV(estimator=LogisticRegression(), param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "  lr_grid.fit(X_train, y_train)\n",
        "  print(lr_grid.best_params_)\n",
        "\n",
        "def gird_param_tunining_dtc(X_train, y_train):\n",
        "  param_grid = {\n",
        "      'criterion': ['gini', 'entropy'],\n",
        "      'max_depth': [None, 5, 10, 15, 20],\n",
        "      'min_samples_split': [2, 4, 6],\n",
        "      'min_samples_leaf': [1, 2, 4],\n",
        "      'max_features': ['auto', 'sqrt', 'log2']\n",
        "  }\n",
        "\n",
        "  dtc_grid = GridSearchCV(estimator=tree.DecisionTreeClassifier(), param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "  dtc_grid.fit(X_train, y_train)\n",
        "  print(dtc_grid.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "id": "_f9eTm-Z74EE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main "
      ],
      "metadata": {
        "id": "7qjAM_yx8cRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "df_games = pd.read_csv('/content/drive/MyDrive/SIAP Data/games.csv')\n",
        "df_stats = pd.read_csv('/content/drive/MyDrive/SIAP Data/League of Legends Champion Stats 12.1.csv', delimiter=';')\n",
        "\n",
        "# Replace champion ids with names\n",
        "champ_map = convert_champion_json_to_map()\n",
        "df_games = merge(champ_map, df_games)\n",
        "\n",
        "# Preprocess data of df_stats\n",
        "df_stats = preprocess_dataframe(df_stats)\n",
        "\n",
        "# Merge read data into single dataframe\n",
        "df_games = merge_games_with_stats(df_games, df_stats)\n",
        "\n",
        "# Leave chosen columns\n",
        "df_games = df_games[[\"winner\", \"t1_champ1win\", \"t2_champ1win\", \"t1_champ2win\", \"t2_champ2win\", \"t1_champ3win\", \"t2_champ3win\", \"t1_champ4win\", \"t2_champ4win\", \"t1_champ5win\", \"t2_champ5win\",\n",
        "                     \"firstBlood\", \"firstTower\"]]\n",
        "\n",
        "# Shuffle data in dataframe\n",
        "df_games = df_games.sample(frac = 1)\n",
        "\n",
        "# Extract y values into separate dataframe, and drop it from existing one\n",
        "df_games = df_games.drop_duplicates()\n",
        "df_games_x, df_games_y = extract_y_from_dataframe(df_games)\n",
        "\n",
        "# Analyzing data\n",
        "# prof = ProfileReport(df_games_x)\n",
        "# prof.to_file(output_file='output_after_dropping_dragon.html')\n",
        "\n",
        "# Split data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_games_x, df_games_y, test_size = 0.30)\n",
        "\n",
        "# Create Standard scaler based on X_train -> transform X_train and X_test using that scaler\n",
        "scaler = create_scaler(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Tune parameters\n",
        "# gird_param_tunining_rfc(X_train, y_train)\n",
        "# gird_param_tunining_svc(X_train, y_train)\n",
        "# gird_param_tunining_lr(X_train, y_train)\n",
        "# gird_param_tunining_dtc(X_train, y_train)\n",
        "\n",
        "# Create models\n",
        "random_forest_classifier = RandomForestClassifier(max_depth=10, min_samples_leaf=4, min_samples_split=6, n_estimators=200) \n",
        "linear_svc = LinearSVC(C=1, dual=True, max_iter=100)\n",
        "logistic_regression = LogisticRegression(C=0.1, max_iter=100, solver='saga', penalty='l2')\n",
        "decision_tree_classifier = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='log2', min_samples_leaf=4, min_samples_split=2)\n",
        "\n",
        "# Train models using X_train and y_train\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "linear_svc.fit(X_train, y_train)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "decision_tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred_1 = random_forest_classifier.predict(X_test)\n",
        "y_pred_2 = linear_svc.predict(X_test)\n",
        "y_pred_3 = logistic_regression.predict(X_test)\n",
        "y_pred_4 = decision_tree_classifier.predict(X_test)\n",
        "\n",
        "print(\"F1 SCORE OF THE RandomForestClassifier: \", metrics.f1_score(y_test, y_pred_1))\n",
        "print(\"F1 SCORE OF THE LinearSVC: \", metrics.f1_score(y_test, y_pred_2))\n",
        "print(\"F1 SCORE OF THE LogisticRegression: \", metrics.f1_score(y_test, y_pred_3))\n",
        "print(\"F1 SCORE OF THE DecisionTreeClassifier: \", metrics.f1_score(y_test, y_pred_4))\n"
      ],
      "metadata": {
        "id": "8kHhsLjE8e_i",
        "outputId": "f3091100-1372-4d62-a0ee-f33930430e35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 SCORE OF THE RandomForestClassifier:  0.713078828685765\n",
            "F1 SCORE OF THE LinearSVC:  0.7127848101265823\n",
            "F1 SCORE OF THE LogisticRegression:  0.7127848101265823\n",
            "F1 SCORE OF THE DecisionTreeClassifier:  0.7130248392384314\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}