{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovansx/siap-project/blob/develop/siap-script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
        "import category_encoders as ce"
      ],
      "metadata": {
        "id": "GZQ4BT2DtkkL",
        "outputId": "94c817f6-cc86-414d-c2ee-5522b309cb07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.0-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (0.13.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.2.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.10.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category_encoders) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "knUWnFxN71cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MinMaxScaler,\n",
        "def convert_champion_json_to_map():\n",
        "  df_champs = pd.read_json('/content/drive/MyDrive/SIAP Data/champion_info.json')\n",
        "  champ_map = {}\n",
        "\n",
        "  for ch in df_champs.data:\n",
        "    champ_map[ch[\"id\"]] = ch[\"name\"]\n",
        "  return champ_map\n",
        "\n",
        "def generate_df2(champ_map, champ_id, champ_name, df1):\n",
        "  df2 = pd.DataFrame(list(champ_map.items()), columns = [champ_id,champ_name])\n",
        "  merged = pd.merge(df1, df2, on=champ_id)\n",
        "  dropped = merged.drop(columns=[champ_id], axis=1)\n",
        "  return dropped\n",
        "\n",
        "def merge(champ_map, df1):\n",
        "  df1 = generate_df2(champ_map, 't1_champ1id', 't1_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ2id', 't1_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ3id', 't1_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ4id', 't1_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't1_champ5id', 't1_champ5name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ1id', 't2_champ1name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ2id', 't2_champ2name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ3id', 't2_champ3name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ4id', 't2_champ4name', df1)\n",
        "  df1 = generate_df2(champ_map, 't2_champ5id', 't2_champ5name', df1)\n",
        "  return df1\n",
        "\n",
        "def delete_columns(df, columns_deletion):\n",
        "  return df.drop(columns=columns_deletion, axis=1)\n",
        "\n",
        "def preprocess_dataframe(df_stats):\n",
        "  df_stats = delete_columns(df_stats, [\"Class\", \"Role\", \"Tier\", \"Trend\", \"Role %\", \"Pick %\", \"Ban %\", \"KDA\"])   # Remove columns\n",
        "  df_stats['Win %'] = df_stats['Win %'].str[:5].astype(float)                                                   # Remove % and convert to float\n",
        "  distinct_names = set(df_stats['Name'])                                                                        # Distinct names\n",
        "  new_df_stats = pd.DataFrame({'Name': [], 'Score': [], 'Win': []})                                             # New empty dataframe of stats\n",
        "\n",
        "  for name in distinct_names:\n",
        "    sub_df_stats = df_stats.loc[df_stats['Name'] == name]\n",
        "    score = sub_df_stats[\"Score\"].mean(axis=0)\n",
        "    win_rate = sub_df_stats[\"Win %\"].mean(axis=0)\n",
        "    one_row_df = pd.DataFrame({\"Name\": [name], \"Score\": [score], \"Win\": [win_rate]})\n",
        "    new_df_stats = new_df_stats.append(one_row_df)                                                              # Append average values for every champion\n",
        "  new_df_stats.index = range(1,len(new_df_stats)+1)                                                             # Set incremental indexes\n",
        "  return new_df_stats\n",
        "\n",
        "def merge_games_with_stats_helper(df_games, df_stats, old_score, new_score, old_win, new_win, name):\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_score, new_score)\n",
        "  df_stats.columns = df_stats.columns.str.replace(old_win, new_win)\n",
        "  df_games.columns = df_games.columns.str.replace(name, \"Name\")\n",
        "  df_games = pd.merge(df_games, df_stats, on=\"Name\")\n",
        "  df_games = df_games.drop(columns=[\"Name\"], axis=1)\n",
        "  return df_games, df_stats\n",
        "\n",
        "def merge_games_with_stats(df_games, df_stats):\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"Score\", \"t1_champ1score\", \"Win\", \"t1_champ1win\", \"t1_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ1score\", \"t1_champ2score\", \"t1_champ1win\", \"t1_champ2win\", \"t1_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ2score\", \"t1_champ3score\", \"t1_champ2win\", \"t1_champ3win\", \"t1_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ3score\", \"t1_champ4score\", \"t1_champ3win\", \"t1_champ4win\", \"t1_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ4score\", \"t1_champ5score\", \"t1_champ4win\", \"t1_champ5win\", \"t1_champ5name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t1_champ5score\", \"t2_champ1score\", \"t1_champ5win\", \"t2_champ1win\", \"t2_champ1name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ1score\", \"t2_champ2score\", \"t2_champ1win\", \"t2_champ2win\", \"t2_champ2name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ2score\", \"t2_champ3score\", \"t2_champ2win\", \"t2_champ3win\", \"t2_champ3name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ3score\", \"t2_champ4score\", \"t2_champ3win\", \"t2_champ4win\", \"t2_champ4name\")\n",
        "  df_games, df_stats = merge_games_with_stats_helper(df_games, df_stats, \"t2_champ4score\", \"t2_champ5score\", \"t2_champ4win\", \"t2_champ5win\", \"t2_champ5name\")\n",
        "  return df_games\n",
        "\n",
        "def create_scaler(df_games):\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit(df_games)\n",
        "  # return (df_games-df_games.mean())/df_games.std()\n",
        "\n",
        "def extract_y_from_dataframe(df_games):\n",
        "  df_games_y = df_games[\"winner\"]\n",
        "  df_games = df_games.drop(columns=[\"winner\"], axis=1)\n",
        "  return df_games, df_games_y\n"
      ],
      "metadata": {
        "id": "_f9eTm-Z74EE"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main "
      ],
      "metadata": {
        "id": "7qjAM_yx8cRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "df_games = pd.read_csv('/content/drive/MyDrive/SIAP Data/games.csv')\n",
        "df_stats = pd.read_csv('/content/drive/MyDrive/SIAP Data/League of Legends Champion Stats 12.1.csv', delimiter=';')\n",
        "\n",
        "# Replace champion ids with names\n",
        "champ_map = convert_champion_json_to_map()\n",
        "df_games = merge(champ_map, df_games)\n",
        "\n",
        "# Preprocess data of df_stats\n",
        "df_stats = preprocess_dataframe(df_stats)\n",
        "\n",
        "# Merge read data into single dataframe\n",
        "df_games = merge_games_with_stats(df_games, df_stats)\n",
        "\n",
        "# Leave chosen columns\n",
        "df_games = df_games[[\"winner\", \"t1_champ1win\", \"t2_champ1win\", \"t1_champ2win\", \"t2_champ2win\", \"t1_champ3win\", \"t2_champ3win\", \"t1_champ4win\", \"t2_champ4win\", \"t1_champ5win\", \"t2_champ5win\",\n",
        "                     \"firstBlood\", \"firstTower\", \"firstDragon\"]]\n",
        "\n",
        "# Shuffle data in dataframe\n",
        "# df_games = df_games.sample(frac = 1)\n",
        "\n",
        "# Extract y values into separate dataframe, and drop it from existing one\n",
        "df_games_x, df_games_y = extract_y_from_dataframe(df_games)\n",
        "\n",
        "first_blood_df = df_games_x[\"firstBlood\"].replace([1, 2, 0], [\"first_team\", \"second_team\", \"equals\"])\n",
        "first_dragon_df = df_games_x[\"firstDragon\"].replace([1, 2, 0], [\"first_team\", \"second_team\", \"equals\"])\n",
        "first_tower_df = df_games_x[\"firstTower\"].replace([1, 2, 0], [\"first_team\", \"second_team\", \"equals\"])\n",
        "# print(df_games_x)\n",
        "first_blood_df_transformed = ce.BinaryEncoder().fit_transform(first_blood_df)\n",
        "first_dragon_df_transformed = ce.BinaryEncoder().fit_transform(first_blood_df)\n",
        "first_tower_df_transformed = ce.BinaryEncoder().fit_transform(first_blood_df)\n",
        "# print(first_blood_df_transformed.head())\n",
        "# first_blood_df = pd.get_dummies(df_games_x.firstBlood, prefix='firstBlood')\n",
        "# first_dragon_df = pd.get_dummies(df_games_x.firstBlood, prefix='firstDragon')\n",
        "# first_tower_df = pd.get_dummies(df_games_x.firstBlood, prefix='firstTower')\n",
        "df_games_x = pd.concat([df_games_x, first_blood_df_transformed, first_dragon_df_transformed, first_tower_df_transformed], axis=1)\n",
        "df_games_x = delete_columns(df_games_x, [\"firstBlood\", \"firstDragon\", \"firstTower\"])\n",
        "print(df_games_x)\n",
        "# Split data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_games_x, df_games_y, test_size = 0.30)\n",
        "# print(df_games_y.describe())\n",
        "# Create Standard scaler based on X_train -> transform X_train and X_test using that scaler\n",
        "scaler = create_scaler(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create model\n",
        "clf = RandomForestClassifier(n_estimators = 100) \n",
        "# clf = LinearSVC(random_state=0, tol=1e-5)\n",
        "# clf = LogisticRegression(max_iter=10000)\n",
        "# clf = tree.DecisionTreeClassifier()\n",
        "\n",
        "# Train model using X_train and y_train\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"F1 SCORE OF THE MODEL: \", metrics.f1_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8kHhsLjE8e_i",
        "outputId": "e1385613-2ad8-498c-bbf2-152d17fa7ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       t1_champ1win  t2_champ1win  t1_champ2win  t2_champ2win  t1_champ3win  \\\n",
            "0            50.350         49.19     50.230000         50.67        52.790   \n",
            "1            51.320         51.85     51.520000         50.09        50.465   \n",
            "2            49.950         51.85     50.100000         50.73        52.850   \n",
            "3            47.935         51.32     49.660000         50.67        52.450   \n",
            "4            49.260         49.28     52.850000         48.45        49.740   \n",
            "...             ...           ...           ...           ...           ...   \n",
            "51485        50.230         50.84     51.630000         49.28        50.530   \n",
            "51486        50.540         49.28     49.260000         52.85        48.880   \n",
            "51487        49.680         50.52     51.030000         51.63        48.165   \n",
            "51488        50.370         52.85     49.283333         49.26        50.180   \n",
            "51489        51.430         50.92     49.885000         49.03        49.280   \n",
            "\n",
            "       t2_champ3win  t1_champ4win  t2_champ4win  t1_champ5win  t2_champ5win  \\\n",
            "0            50.370     49.740000     49.750000        50.450         48.88   \n",
            "1            51.550     50.470000     49.750000        49.885         48.88   \n",
            "2            48.090     50.370000     49.750000        49.740         48.88   \n",
            "3            49.740     49.560000     49.750000        49.920         48.88   \n",
            "4            49.860     52.316667     49.750000        50.950         48.88   \n",
            "...             ...           ...           ...           ...           ...   \n",
            "51485        52.740     50.430000     51.436667        48.450         50.47   \n",
            "51486        49.625     48.670000     48.165000        50.830         50.47   \n",
            "51487        49.260     49.466667     49.985000        49.030         50.47   \n",
            "51488        48.165     51.320000     50.950000        50.230         50.47   \n",
            "51489        52.140     50.230000     50.670000        52.740         50.47   \n",
            "\n",
            "       firstBlood_0  firstBlood_1  firstBlood_0  firstBlood_1  firstBlood_0  \\\n",
            "0                 0             1             0             1             0   \n",
            "1                 1             0             1             0             1   \n",
            "2                 1             0             1             0             1   \n",
            "3                 0             1             0             1             0   \n",
            "4                 0             1             0             1             0   \n",
            "...             ...           ...           ...           ...           ...   \n",
            "51485             0             1             0             1             0   \n",
            "51486             1             0             1             0             1   \n",
            "51487             1             0             1             0             1   \n",
            "51488             0             1             0             1             0   \n",
            "51489             1             0             1             0             1   \n",
            "\n",
            "       firstBlood_1  \n",
            "0                 1  \n",
            "1                 0  \n",
            "2                 0  \n",
            "3                 1  \n",
            "4                 1  \n",
            "...             ...  \n",
            "51485             1  \n",
            "51486             0  \n",
            "51487             0  \n",
            "51488             1  \n",
            "51489             0  \n",
            "\n",
            "[51490 rows x 16 columns]\n",
            "F1 SCORE OF THE MODEL:  0.5993970343936504\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}